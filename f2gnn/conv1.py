import torch
from torch_geometric.nn import MessagePassing
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool, global_add_pool
from ogb.graphproppred.mol_encoder import AtomEncoder
from torch_geometric.utils import degree
from typing import Union, Tuple, Optional, Callable
from torch_geometric.typing import PairTensor
from torch_geometric.nn.dense.linear import Linear
from torch import Tensor
from torch.nn import Parameter
from torch_geometric.utils import remove_self_loops, add_self_loops, softmax
# from inits import glorot, zeros, reset
from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,
                                    OptTensor)
from torch_sparse import SparseTensor, set_diag, matmul
# from DeeperGCN_with_HIG.gcn_lib.sparse.torch_message import GenMessagePassing, MsgNorm
# from DeeperGCN_with_HIG.gcn_lib.sparse.torch_nn import MLP, act_layer, norm_layer, BondEncoder
from torch.nn import ModuleList, Sequential, ReLU, PReLU, Tanh, BatchNorm1d as BN
import math
import pdb

class GINEPLUS(MessagePassing):
    def __init__(self, fun, dim, k=4, **kwargs):
        super().__init__(aggr='add')
        self.k = k
        self.nn = fun
        self.eps = torch.nn.Parameter(torch.zeros(k + 1, dim), requires_grad=True)
        self.edge_encoder = torch.nn.Linear(7, dim)

    def forward(self, XX, multihop_edge_index, distance, edge_attr):
        """Warning, XX is now a list of previous xs, with x[0] being the last layer"""

        edge_attr = self.edge_encoder(edge_attr)

        assert len(XX) >= self.k
        assert XX[-1].size(-1) == edge_attr.size(-1)
        result = (1 + self.eps[0]) * XX[0]

        for i, x in enumerate(XX):
            if i >= self.k:
                break
            if i == 0:
                out = self.propagate(multihop_edge_index[:, distance == i + 1], edge_attr=edge_attr, x=x)
            else:
                out = self.propagate(multihop_edge_index[:, distance == i + 1], edge_attr=None, x=x)
            result += (1 + self.eps[i + 1]) * out
        result = self.nn(result)
        return [result] + XX

    def message(self, x_j, edge_attr):
        if edge_attr is not None:
            return F.relu(x_j + edge_attr)
        else:
            return F.relu(x_j)

class ExpC(MessagePassing):
    def __init__(self, hidden, num_aggr, config, **kwargs):
        super(ExpC, self).__init__(aggr='add', **kwargs)
        self.hidden = hidden
        self.num_aggr = num_aggr

        self.fea_mlp = Sequential(
            Linear(hidden * self.num_aggr, hidden),
            PReLU(),
            Linear(hidden, hidden),
            PReLU())

        self.aggr_mlp = Sequential(
            Linear(hidden * 2, self.num_aggr),
            Tanh())

        self.BN = BN(hidden)

        self.edge_encoder = torch.nn.Linear(7, hidden)

    def forward(self, x, edge_index, edge_attr):
        edge_attr = self.edge_encoder(edge_attr)
        edge_index, _ = remove_self_loops(edge_index)
        # edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))
        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)
        if self.BN is not None:
            out = self.BN(out)
        return out

    def message(self, x_i, x_j, edge_attr):
        xe = x_j + edge_attr
        aggr_emb = self.aggr_mlp(torch.cat([x_i, xe], dim=-1))
        feature2d = torch.matmul(aggr_emb.unsqueeze(-1), xe.unsqueeze(-1)
                                 .transpose(-1, -2)).squeeze(-1).view(-1, self.hidden * self.num_aggr)
        return self.fea_mlp(feature2d)

    def update(self, aggr_out, x):
        root_emb = self.aggr_mlp(torch.cat([x, x], dim=-1))
        feature2d = torch.matmul(root_emb.unsqueeze(-1), x.unsqueeze(-1)
                                 .transpose(-1, -2)).squeeze(-1).view(-1, self.hidden * self.num_aggr)
        return aggr_out + self.fea_mlp(feature2d)

    def __repr__(self):
        return self.__class__.__name__

class ExpC_star(MessagePassing):
    def __init__(self, hidden, num_aggr, config, **kwargs):
        super(ExpC_star, self).__init__(aggr='add', **kwargs)
        self.hidden = hidden
        self.num_aggr = num_aggr

        self.fea_mlp = Sequential(
            Linear(hidden * self.num_aggr, hidden),
            ReLU(),
            Linear(hidden, hidden),
            ReLU())

        self.aggr_mlp = Sequential(
            Linear(hidden * 2, self.num_aggr),
            Tanh())

        self.BN = BN(hidden)

        self.edge_encoder = torch.nn.Linear(7, hidden)

    def forward(self, x, edge_index, edge_attr):
        edge_attr = self.edge_encoder(edge_attr)
        edge_index, _ = remove_self_loops(edge_index)
        # edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))
        out = self.fea_mlp(self.propagate(edge_index, x=x, edge_attr=edge_attr))
        if self.BN is not None:
            out = self.BN(out)
        return out

    def message(self, x_i, x_j, edge_attr):
        xe = x_j + edge_attr
        aggr_emb = self.aggr_mlp(torch.cat([x_i, xe], dim=-1))
        feature2d = torch.matmul(aggr_emb.unsqueeze(-1), xe.unsqueeze(-1)
                                 .transpose(-1, -2)).squeeze(-1).view(-1, self.hidden * self.num_aggr)
        return feature2d

    def update(self, aggr_out, x):
        root_emb = self.aggr_mlp(torch.cat([x, x], dim=-1))
        feature2d = torch.matmul(root_emb.unsqueeze(-1), x.unsqueeze(-1)
                                 .transpose(-1, -2)).squeeze(-1).view(-1, self.hidden * self.num_aggr)
        return aggr_out + feature2d

    def __repr__(self):
        return self.__class__.__name__

### GIN convolution along the graph structure
class GINConv(MessagePassing):
    def __init__(self, emb_dim):
        '''
            emb_dim (int): node embedding dimensionality
        '''

        super(GINConv, self).__init__(aggr="add")

        self.mlp = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2 * emb_dim), torch.nn.BatchNorm1d(2 * emb_dim),
                                       torch.nn.ReLU(), torch.nn.Linear(2 * emb_dim, emb_dim))
        self.eps = torch.nn.Parameter(torch.Tensor([0]))

        # self.bond_encoder = BondEncoder(emb_dim=emb_dim)
        self.bond_encoder = torch.nn.Linear(7, emb_dim)

    def forward(self, x, edge_index, edge_attr):
        edge_embedding = self.bond_encoder(edge_attr)
        out = self.mlp((1 + self.eps) * x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))

        return out

    def message(self, x_j, edge_attr):
        return F.relu(x_j + edge_attr)

    def update(self, aggr_out):
        return aggr_out


### GCN convolution along the graph structure
class GCNConv(MessagePassing):
    def __init__(self, emb_dim):
        super(GCNConv, self).__init__(aggr='add')

        self.linear = torch.nn.Linear(emb_dim, emb_dim)
        self.root_emb = torch.nn.Embedding(1, emb_dim)
        self.bond_encoder = torch.nn.Linear(7, emb_dim)

    def forward(self, x, edge_index, edge_attr):
        x = self.linear(x)

        edge_embedding = self.bond_encoder(edge_attr)

        row, col = edge_index

        # edge_weight = torch.ones((edge_index.size(1), ), device=edge_index.device)
        deg = degree(row, x.size(0), dtype=x.dtype) + 1
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0

        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

        return self.propagate(edge_index, x=x, edge_attr=edge_embedding, norm=norm) + F.relu(
            x + self.root_emb.weight) * 1. / deg.view(-1, 1)

    def message(self, x_j, edge_attr, norm):
        return norm.view(-1, 1) * F.relu(x_j + edge_attr)

    def update(self, aggr_out):
        return aggr_out

class TransformerConv(MessagePassing):

    _alpha: OptTensor

    def __init__(
        self,
        in_channels: Union[int, Tuple[int, int]],
        out_channels: int,
        heads: int = 1,
        concat: bool = True,
        beta: bool = False,
        dropout: float = 0.,
        edge_dim: Optional[int] = None,
        bias: bool = True,
        root_weight: bool = True,
        **kwargs,
    ):
        kwargs.setdefault('aggr', 'add')
        super(TransformerConv, self).__init__(node_dim=0, **kwargs)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.heads = heads
        self.beta = beta and root_weight
        self.root_weight = root_weight
        self.concat = concat
        self.dropout = dropout
        self.edge_dim = edge_dim
        # self.bond_encoder = BondEncoder(emb_dim=heads * out_channels)
        self.bond_encoder = torch.nn.Linear(7, heads * out_channels)

        if isinstance(in_channels, int):
            in_channels = (in_channels, in_channels)

        self.lin_key = Linear(in_channels[0], heads * out_channels)
        self.lin_query = Linear(in_channels[1], heads * out_channels)
        self.lin_value = Linear(in_channels[0], heads * out_channels)
        if edge_dim is not None:
            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)
        else:
            self.lin_edge = self.register_parameter('lin_edge', None)

        if concat:
            self.lin_skip = Linear(in_channels[1], heads * out_channels,
                                   bias=bias)
            if self.beta:
                self.lin_beta = Linear(3 * heads * out_channels, 1, bias=False)
            else:
                self.lin_beta = self.register_parameter('lin_beta', None)
        else:
            self.lin_skip = Linear(in_channels[1], out_channels, bias=bias)
            if self.beta:
                self.lin_beta = Linear(3 * out_channels, 1, bias=False)
            else:
                self.lin_beta = self.register_parameter('lin_beta', None)

        self.reset_parameters()

    def reset_parameters(self):
        self.lin_key.reset_parameters()
        self.lin_query.reset_parameters()
        self.lin_value.reset_parameters()
        if self.edge_dim:
            self.lin_edge.reset_parameters()
        self.lin_skip.reset_parameters()
        if self.beta:
            self.lin_beta.reset_parameters()

    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,
                edge_attr: OptTensor = None, return_attention_weights=None):
        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, NoneType) -> Tensor  # noqa
        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, NoneType) -> Tensor  # noqa
        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa
        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, bool) -> Tuple[Tensor, SparseTensor]  # noqa
        r"""
        Args:
            return_attention_weights (bool, optional): If set to :obj:`True`,
                will additionally return the tuple
                :obj:`(edge_index, attention_weights)`, holding the computed
                attention weights for each edge. (default: :obj:`None`)
        """

        edge_attr = self.bond_encoder(edge_attr)
        if isinstance(x, Tensor):
            x: PairTensor = (x, x)

        # propagate_type: (x: PairTensor, edge_attr: OptTensor)
        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)

        alpha = self._alpha
        self._alpha = None

        if self.concat:
            out = out.view(-1, self.heads * self.out_channels)
        else:
            out = out.mean(dim=1)

        if self.root_weight:
            x_r = self.lin_skip(x[1])
            if self.lin_beta is not None:
                beta = self.lin_beta(torch.cat([out, x_r, out - x_r], dim=-1))
                beta = beta.sigmoid()
                out = beta * x_r + (1 - beta) * out
            else:
                out += x_r

        if isinstance(return_attention_weights, bool):
            assert alpha is not None
            if isinstance(edge_index, Tensor):
                return out, (edge_index, alpha)
            elif isinstance(edge_index, SparseTensor):
                return out, edge_index.set_value(alpha, layout='coo')
        else:
            return out

    def message(self, x_i: Tensor, x_j: Tensor, edge_attr: OptTensor,
                index: Tensor, ptr: OptTensor,
                size_i: Optional[int]) -> Tensor:

        query = self.lin_query(x_i).view(-1, self.heads, self.out_channels)
        key = self.lin_key(x_j).view(-1, self.heads, self.out_channels)

        if self.lin_edge is not None:
            assert edge_attr is not None
            edge_attr = self.lin_edge(edge_attr).view(-1, self.heads,
                                                      self.out_channels)
            key += edge_attr

        alpha = (query * key).sum(dim=-1) / math.sqrt(self.out_channels)
        alpha = softmax(alpha, index, ptr, size_i)
        self._alpha = alpha
        alpha = F.dropout(alpha, p=self.dropout, training=self.training)

        out = self.lin_value(x_j).view(-1, self.heads, self.out_channels)
        if edge_attr is not None:
            out += edge_attr

        out *= alpha.view(-1, self.heads, 1)
        return out

    def __repr__(self):
        return '{}({}, {}, heads={})'.format(self.__class__.__name__,
                                             self.in_channels,
                                             self.out_channels, self.heads)



class GATConv(MessagePassing):

    _alpha: OptTensor

    def __init__(
        self,
        in_channels: Union[int, Tuple[int, int]],
        out_channels: int,
        heads: int = 1,
        concat: bool = True,
        negative_slope: float = 0.2,
        dropout: float = 0.0,
        add_self_loops: bool = False,
        edge_dim: Optional[int] = None,
        fill_value: Union[float, Tensor, str] = 'mean',
        bias: bool = True,
        **kwargs,
    ):
        kwargs.setdefault('aggr', 'add')
        super().__init__(node_dim=0, **kwargs)

# #         self.in_channels = in_channels
# #         self.out_channels = out_channels
# #         self.heads = heads
# #         self.concat = concat
# #         self.negative_slope = negative_slope
# #         self.dropout = dropout
# #         self.add_self_loops = add_self_loops
# #         self.edge_dim = edge_dim
# #         self.fill_value = fill_value
# #         # self.bond_encoder = BondEncoder(emb_dim=heads * out_channels)
# #         self.bond_encoder = torch.nn.Linear(7, heads * out_channels)

# #         # In case we are operating in bipartite graphs, we apply separate
# #         # transformations 'lin_src' and 'lin_dst' to source and target nodes:
# #         if isinstance(in_channels, int):
# #             self.lin_src = Linear(in_channels, heads * out_channels,
# #                                   bias=False, weight_initializer='glorot')
# #             self.lin_dst = self.lin_src
# #         else:
# #             self.lin_src = Linear(in_channels[0], heads * out_channels, False,
# #                                   weight_initializer='glorot')
# #             self.lin_dst = Linear(in_channels[1], heads * out_channels, False,
# #                                   weight_initializer='glorot')

# #         # The learnable parameters to compute attention coefficients:
# #         self.att_src = Parameter(torch.Tensor(1, heads, out_channels))
# #         self.att_dst = Parameter(torch.Tensor(1, heads, out_channels))

# #         if edge_dim is not None:
# #             self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False,
# #                                    weight_initializer='glorot')
# #             self.att_edge = Parameter(torch.Tensor(1, heads, out_channels))
# #         else:
# #             self.lin_edge = None
# #             self.register_parameter('att_edge', None)

# #         if bias and concat:
# #             self.bias = Parameter(torch.Tensor(heads * out_channels))
# #         elif bias and not concat:
# #             self.bias = Parameter(torch.Tensor(out_channels))
# #         else:
# #             self.register_parameter('bias', None)

# #         self._alpha = None

# #         self.reset_parameters()

# #     def reset_parameters(self):
# #         self.lin_src.reset_parameters()
# #         self.lin_dst.reset_parameters()
# #         if self.lin_edge is not None:
# #             self.lin_edge.reset_parameters()
# #         glorot(self.att_src)
# #         glorot(self.att_dst)
# #         glorot(self.att_edge)
# #         zeros(self.bias)

# #     def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,
# #                 edge_attr: OptTensor = None, size: Size = None,
# #                 return_attention_weights=None):
# #         # type: (Union[Tensor, OptPairTensor], Tensor, OptTensor, Size, NoneType) -> Tensor  # noqa
# #         # type: (Union[Tensor, OptPairTensor], SparseTensor, OptTensor, Size, NoneType) -> Tensor  # noqa
# #         # type: (Union[Tensor, OptPairTensor], Tensor, OptTensor, Size, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa
# #         # type: (Union[Tensor, OptPairTensor], SparseTensor, OptTensor, Size, bool) -> Tuple[Tensor, SparseTensor]  # noqa
# #         r"""
# #         Args:
# #             return_attention_weights (bool, optional): If set to :obj:`True`,
# #                 will additionally return the tuple
# #                 :obj:`(edge_index, attention_weights)`, holding the computed
# #                 attention weights for each edge. (default: :obj:`None`)
# #         """
# #         H, C = self.heads, self.out_channels
# #         # edge_attr = self.bond_encoder(edge_attr)

# #         # We first transform the input node features. If a tuple is passed, we
# #         # transform source and target node features via separate weights:
# #         if isinstance(x, Tensor):
# #             assert x.dim() == 2, "Static graphs not supported in 'GATConv'"
# #             x_src = x_dst = self.lin_src(x).view(-1, H, C)
# #         else:  # Tuple of source and target node features:
# #             x_src, x_dst = x
# #             assert x_src.dim() == 2, "Static graphs not supported in 'GATConv'"
# #             x_src = self.lin_src(x_src).view(-1, H, C)
# #             if x_dst is not None:
# #                 x_dst = self.lin_dst(x_dst).view(-1, H, C)

# #         x = (x_src, x_dst)

# #         # Next, we compute node-level attention coefficients, both for source
# #         # and target nodes (if present):
# #         alpha_src = (x_src * self.att_src).sum(dim=-1)
# #         alpha_dst = None if x_dst is None else (x_dst * self.att_dst).sum(-1)
# #         alpha = (alpha_src, alpha_dst)

# #         if self.add_self_loops:
# #             if isinstance(edge_index, Tensor):
# #                 # We only want to add self-loops for nodes that appear both as
# #                 # source and target nodes:
# #                 num_nodes = x_src.size(0)
# #                 if x_dst is not None:
# #                     num_nodes = min(num_nodes, x_dst.size(0))
# #                 num_nodes = min(size) if size is not None else num_nodes
# #                 edge_index, edge_attr = remove_self_loops(
# #                     edge_index, edge_attr)
# #                 edge_index, edge_attr = add_self_loops(
# #                     edge_index, edge_attr, fill_value=self.fill_value,
# #                     num_nodes=num_nodes)
# #             elif isinstance(edge_index, SparseTensor):
# #                 if self.edge_dim is None:
# #                     edge_index = set_diag(edge_index)
# #                 else:
# #                     raise NotImplementedError(
# #                         "The usage of 'edge_attr' and 'add_self_loops' "
# #                         "simultaneously is currently not yet supported for "
# #                         "'edge_index' in a 'SparseTensor' form")

# #         # propagate_type: (x: OptPairTensor, alpha: OptPairTensor, edge_attr: OptTensor)  # noqa
# #         out = self.propagate(edge_index, x=x, alpha=alpha, edge_attr=edge_attr,
# #                              size=size)

# #         alpha = self._alpha
# #         assert alpha is not None
# #         self._alpha = None

# #         if self.concat:
# #             out = out.view(-1, self.heads * self.out_channels)
# #         else:
# #             out = out.mean(dim=1)

# #         if self.bias is not None:
# #             out += self.bias

# #         if isinstance(return_attention_weights, bool):
# #             if isinstance(edge_index, Tensor):
# #                 return out, (edge_index, alpha)
# #             elif isinstance(edge_index, SparseTensor):
# #                 return out, edge_index.set_value(alpha, layout='coo')
# #         else:
# #             return out

# #     def message(self, x_j: Tensor, alpha_j: Tensor, alpha_i: OptTensor,
# #                 edge_attr: OptTensor, index: Tensor, ptr: OptTensor,
# #                 size_i: Optional[int]) -> Tensor:
# #         # Given edge-level attention coefficients for source and target nodes,
# #         # we simply need to sum them up to "emulate" concatenation:
# #         alpha = alpha_j if alpha_i is None else alpha_j + alpha_i

# #         if edge_attr is not None:
# #             if edge_attr.dim() == 1:
# #                 edge_attr = edge_attr.view(-1, 1)
# #             assert self.bond_encoder is not None
# #             # edge_attr = self.lin_edge(edge_attr)
# #             edge_attr = self.bond_encoder(edge_attr)
# #             edge_attr = edge_attr.view(-1, self.heads, self.out_channels)
# #             alpha_edge = (edge_attr * self.att_edge).sum(dim=-1)
# #             alpha = alpha + alpha_edge

# #         alpha = F.leaky_relu(alpha, self.negative_slope)
# #         alpha = softmax(alpha, index, ptr, size_i)
# #         self._alpha = alpha  # Save for later use.
# #         alpha = F.dropout(alpha, p=self.dropout, training=self.training)
# #         return x_j * alpha.unsqueeze(-1)

# #     def __repr__(self):
# #         return '{}({}, {}, heads={})'.format(self.__class__.__name__,
# #                                              self.in_channels,
# #                                              self.out_channels, self.heads)


class MFConv(MessagePassing):
    r"""The graph neural network operator from the
    `"Convolutional Networks on Graphs for Learning Molecular Fingerprints"
    <https://arxiv.org/abs/1509.09292>`_ paper

    .. math::
        \mathbf{x}^{\prime}_i = \mathbf{W}^{(\deg(i))}_1 \mathbf{x}_i +
        \mathbf{W}^{(\deg(i))}_2 \sum_{j \in \mathcal{N}(i)} \mathbf{x}_j

    which trains a distinct weight matrix for each possible vertex degree.

    Args:
        in_channels (int or tuple): Size of each input sample, or :obj:`-1` to
            derive the size from the first input(s) to the forward method.
            A tuple corresponds to the sizes of source and target
            dimensionalities.
        out_channels (int): Size of each output sample.
        max_degree (int, optional): The maximum node degree to consider when
            updating weights (default: :obj:`10`)
        bias (bool, optional): If set to :obj:`False`, the layer will not learn
            an additive bias. (default: :obj:`True`)
        **kwargs (optional): Additional arguments of
            :class:`torch_geometric.nn.conv.MessagePassing`.
    """
    def __init__(self, in_channels: Union[int, Tuple[int, int]],
                 out_channels: int, max_degree: int = 10, bias=True, **kwargs):
        kwargs.setdefault('aggr', 'add')
        super(MFConv, self).__init__(**kwargs)

#         self.in_channels = in_channels
#         self.out_channels = out_channels
#         self.max_degree = max_degree
#         # self.bond_encoder = BondEncoder(emb_dim=out_channels)
#         self.bond_encoder = torch.nn.Linear(7, out_channels)

#         if isinstance(in_channels, int):
#             in_channels = (in_channels, in_channels)

#         self.lins_l = ModuleList([
#             Linear(in_channels[0], out_channels, bias=bias)
#             for _ in range(max_degree + 1)
#         ])

#         self.lins_r = ModuleList([
#             Linear(in_channels[1], out_channels, bias=False)
#             for _ in range(max_degree + 1)
#         ])

#         self.reset_parameters()

#     def reset_parameters(self):
#         for lin in self.lins_l:
#             lin.reset_parameters()
#         for lin in self.lins_r:
#             lin.reset_parameters()

#     def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj, edge_attr: Tensor,
#                 size: Size = None, ) -> Tensor:
#         """"""
#         if isinstance(x, Tensor):
#             x: OptPairTensor = (x, x)
#         x_r = x[1]
#         edge_attr = self.bond_encoder(edge_attr)
#         deg = x[0]  # Dummy.
#         if isinstance(edge_index, SparseTensor):
#             deg = edge_index.storage.rowcount()
#         elif isinstance(edge_index, Tensor):
#             i = 1 if self.flow == 'source_to_target' else 0
#             N = x[0].size(self.node_dim)
#             N = size[1] if size is not None else N
#             N = x_r.size(self.node_dim) if x_r is not None else N
#             deg = degree(edge_index[i], N, dtype=torch.long)
#         deg.clamp_(max=self.max_degree)

#         # propagate_type: (x: OptPairTensor)
#         h = self.propagate(edge_index, x=x, size=size, edge_attr=edge_attr)

#         out = h.new_empty(list(h.size())[:-1] + [self.out_channels])
#         for i, (lin_l, lin_r) in enumerate(zip(self.lins_l, self.lins_r)):
#             idx = (deg == i).nonzero().view(-1)
#             r = lin_l(h.index_select(self.node_dim, idx))

#             if x_r is not None:
#                 r += lin_r(x_r.index_select(self.node_dim, idx))

#             out.index_copy_(self.node_dim, idx, r)

#         return out

#     def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:
#         return x_j + edge_attr

#     def message_and_aggregate(self, adj_t: SparseTensor,
#                               x: OptPairTensor) -> Tensor:
#         adj_t = adj_t.set_value(None, layout=None)
#         return matmul(adj_t, x[0], reduce=self.aggr)

#     def __repr__(self):
#         return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,
#                                    self.out_channels)


class GINEConv(MessagePassing):

    def __init__(self, nn: Callable, eps: float = 0., train_eps: bool = False,
                 edge_dim: Optional[int] = None, **kwargs):
        kwargs.setdefault('aggr', 'add')
        super(GINEConv, self).__init__(**kwargs)
#         self.nn = nn
#         self.initial_eps = eps
#         # self.bond_encoder = BondEncoder(emb_dim=self.nn[0].in_channels)
#         self.bond_encoder = torch.nn.Linear(7, self.nn[0].in_channels)

#         if train_eps:
#             self.eps = torch.nn.Parameter(torch.Tensor([eps]))
#         else:
#             self.register_buffer('eps', torch.Tensor([eps]))
#         if edge_dim is not None:
#             if hasattr(self.nn[0], 'in_features'):
#                 in_channels = self.nn[0].in_features
#             else:
#                 in_channels = self.nn[0].in_channels
#             self.lin = Linear(edge_dim, in_channels)
#         else:
#             self.lin = None
#         self.reset_parameters()

#     def reset_parameters(self):
#         reset(self.nn)
#         self.eps.data.fill_(self.initial_eps)
#         if self.lin is not None:
#             self.lin.reset_parameters()

#     def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,
#                 edge_attr: OptTensor = None, size: Size = None) -> Tensor:
#         """"""
#         if isinstance(x, Tensor):
#             x: OptPairTensor = (x, x)

#         # propagate_type: (x: OptPairTensor, edge_attr: OptTensor)
#         edge_attr = self.bond_encoder(edge_attr)
#         out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)

#         x_r = x[1]
#         if x_r is not None:
#             out += (1 + self.eps) * x_r

#         return self.nn(out)

#     def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:
#         if self.lin is None and x_j.size(-1) != edge_attr.size(-1):
#             raise ValueError("Node and edge feature dimensionalities do not "
#                              "match. Consider setting the 'edge_dim' "
#                              "attribute of 'GINEConv'")

#         if self.lin is not None:
#             edge_attr = self.lin(edge_attr)

#         return (x_j + edge_attr).relu()

#     def __repr__(self):
#         return '{}(nn={})'.format(self.__class__.__name__, self.nn)

# class GENConv(GenMessagePassing):
class GENConv():
    """
     GENeralized Graph Convolution (GENConv): https://arxiv.org/pdf/2006.07739.pdf
     SoftMax  &  PowerMean Aggregation
    """
    def __init__(self, in_dim, emb_dim,
                 aggr='softmax',
                 t=1.0, learn_t=True,
                 p=1.0, learn_p=False,
                 y=0.0, learn_y=False,
                 msg_norm=False, learn_msg_scale=False,
                 encode_edge=True, bond_encoder=True,
                 edge_feat_dim=None,
                 norm='batch', mlp_layers=1,
                 eps=1e-7):

        super(GENConv, self).__init__(aggr=aggr,
                                      t=t, learn_t=learn_t,
                                      p=p, learn_p=learn_p,
                                      y=y, learn_y=learn_y)

#         channels_list = [in_dim]

#         for i in range(mlp_layers-1):
#             channels_list.append(in_dim*2)

#         channels_list.append(emb_dim)

#         self.mlp = MLP(channels=channels_list,
#                        norm=norm,
#                        last_lin=True)

#         self.msg_encoder = torch.nn.ReLU()
#         self.eps = eps

#         self.msg_norm = msg_norm
#         self.encode_edge = encode_edge
#         self.bond_encoder = bond_encoder

#         if msg_norm:
#             self.msg_norm = MsgNorm(learn_msg_scale=learn_msg_scale)
#         else:
#             self.msg_norm = None

#         if self.encode_edge:
#             if self.bond_encoder:
#                 # self.edge_encoder = BondEncoder(emb_dim=in_dim)
#                 self.edge_encoder = torch.nn.Linear(7, in_dim)
#             else:
#                 self.edge_encoder = torch.nn.Linear(edge_feat_dim, in_dim)

#     def forward(self, x, edge_index, edge_attr=None):
#         if self.encode_edge and edge_attr is not None:
#             edge_emb = self.edge_encoder(edge_attr)
#         else:
#             edge_emb = edge_attr

#         m = self.propagate(edge_index, x=x, edge_attr=edge_emb)

#         if self.msg_norm is not None:
#             m = self.msg_norm(x, m)

#         h = x + m
#         out = self.mlp(h)

#         return out

#     def message(self, x_j, edge_attr=None):

#         if edge_attr is not None:
#             msg = x_j + edge_attr
#         else:
#             msg = x_j

#         return self.msg_encoder(msg) + self.eps

#     def update(self, aggr_out):
#         return aggr_out

class GATv2Conv(MessagePassing):
    r"""The GATv2 operator from the `"How Attentive are Graph Attention Networks?"
    <https://arxiv.org/abs/2105.14491>`_ paper, which fixes the static
    attention problem of the standard :class:`~torch_geometric.conv.GATConv`
    layer: since the linear layers in the standard GAT are applied right after
    each other, the ranking of attended nodes is unconditioned on the query
    node. In contrast, in GATv2, every node can attend to any other node.

    .. math::
        \mathbf{x}^{\prime}_i = \alpha_{i,i}\mathbf{\Theta}\mathbf{x}_{i} +
        \sum_{j \in \mathcal{N}(i)} \alpha_{i,j}\mathbf{\Theta}\mathbf{x}_{j},

    where the attention coefficients :math:`\alpha_{i,j}` are computed as

    .. math::
        \alpha_{i,j} =
        \frac{
        \exp\left(\mathbf{a}^{\top}\mathrm{LeakyReLU}\left(\mathbf{\Theta}
        [\mathbf{x}_i \, \Vert \, \mathbf{x}_j]
        \right)\right)}
        {\sum_{k \in \mathcal{N}(i) \cup \{ i \}}
        \exp\left(\mathbf{a}^{\top}\mathrm{LeakyReLU}\left(\mathbf{\Theta}
        [\mathbf{x}_i \, \Vert \, \mathbf{x}_k]
        \right)\right)}.

    If the graph has multi-dimensional edge features :math:`\mathbf{e}_{i,j}`,
    the attention coefficients :math:`\alpha_{i,j}` are computed as

    .. math::
        \alpha_{i,j} =
        \frac{
        \exp\left(\mathbf{a}^{\top}\mathrm{LeakyReLU}\left(\mathbf{\Theta}
        [\mathbf{x}_i \, \Vert \, \mathbf{x}_j \, \Vert \, \mathbf{e}_{i,j}]
        \right)\right)}
        {\sum_{k \in \mathcal{N}(i) \cup \{ i \}}
        \exp\left(\mathbf{a}^{\top}\mathrm{LeakyReLU}\left(\mathbf{\Theta}
        [\mathbf{x}_i \, \Vert \, \mathbf{x}_k \, \Vert \, \mathbf{e}_{i,k}]
        \right)\right)}.

    Args:
        in_channels (int): Size of each input sample, or :obj:`-1` to derive
            the size from the first input(s) to the forward method.
        out_channels (int): Size of each output sample.
        heads (int, optional): Number of multi-head-attentions.
            (default: :obj:`1`)
        concat (bool, optional): If set to :obj:`False`, the multi-head
            attentions are averaged instead of concatenated.
            (default: :obj:`True`)
        negative_slope (float, optional): LeakyReLU angle of the negative
            slope. (default: :obj:`0.2`)
        dropout (float, optional): Dropout probability of the normalized
            attention coefficients which exposes each node to a stochastically
            sampled neighborhood during training. (default: :obj:`0`)
        add_self_loops (bool, optional): If set to :obj:`False`, will not add
            self-loops to the input graph. (default: :obj:`True`)
        edge_dim (int, optional): Edge feature dimensionality (in case
            there are any). (default: :obj:`None`)
        fill_value (float or Tensor or str, optional): The way to generate
            edge features of self-loops (in case :obj:`edge_dim != None`).
            If given as :obj:`float` or :class:`torch.Tensor`, edge features of
            self-loops will be directly given by :obj:`fill_value`.
            If given as :obj:`str`, edge features of self-loops are computed by
            aggregating all features of edges that point to the specific node,
            according to a reduce operation. (:obj:`"add"`, :obj:`"mean"`,
            :obj:`"min"`, :obj:`"max"`, :obj:`"mul"`). (default: :obj:`"mean"`)
        bias (bool, optional): If set to :obj:`False`, the layer will not learn
            an additive bias. (default: :obj:`True`)
        share_weights (bool, optional): If set to :obj:`True`, the same matrix
            will be applied to the source and the target node of every edge.
            (default: :obj:`False`)
        **kwargs (optional): Additional arguments of
            :class:`torch_geometric.nn.conv.MessagePassing`.
    """
    _alpha: OptTensor

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        heads: int = 1,
        concat: bool = True,
        negative_slope: float = 0.2,
        dropout: float = 0.0,
        add_self_loops: bool = False,
        edge_dim: Optional[int] = None,
        fill_value: Union[float, Tensor, str] = 'mean',
        bias: bool = True,
        share_weights: bool = False,
        **kwargs,
    ):
        super(GATv2Conv, self).__init__(node_dim=0, **kwargs)

#         self.in_channels = in_channels
#         self.out_channels = out_channels
#         self.heads = heads
#         # self.bond_encoder = BondEncoder(in_channels)
#         self.bond_encoder = torch.nn.Linear(7, in_channels)

#         self.concat = concat
#         self.negative_slope = negative_slope
#         self.dropout = dropout
#         self.add_self_loops = add_self_loops
#         self.edge_dim = edge_dim
#         self.fill_value = fill_value
#         self.share_weights = share_weights

#         self.lin_l = Linear(in_channels, heads * out_channels, bias=bias,
#                             weight_initializer='glorot')
#         if share_weights:
#             self.lin_r = self.lin_l
#         else:
#             self.lin_r = Linear(in_channels, heads * out_channels, bias=bias,
#                                 weight_initializer='glorot')

#         self.att = Parameter(torch.Tensor(1, heads, out_channels))

#         if edge_dim is not None:
#             self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False,
#                                    weight_initializer='glorot')
#         else:
#             self.lin_edge = None

#         if bias and concat:
#             self.bias = Parameter(torch.Tensor(heads * out_channels))
#         elif bias and not concat:
#             self.bias = Parameter(torch.Tensor(out_channels))
#         else:
#             self.register_parameter('bias', None)

#         self._alpha = None

#         self.reset_parameters()

#     def reset_parameters(self):
#         self.lin_l.reset_parameters()
#         self.lin_r.reset_parameters()
#         if self.lin_edge is not None:
#             self.lin_edge.reset_parameters()
#         glorot(self.att)
#         zeros(self.bias)


#     def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,
#                 edge_attr: OptTensor = None, size: Size = None,
#                 return_attention_weights: bool = None):
#         # type: (Union[Tensor, PairTensor], Tensor, OptTensor, Size, NoneType) -> Tensor  # noqa
#         # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, Size, NoneType) -> Tensor  # noqa
#         # type: (Union[Tensor, PairTensor], Tensor, OptTensor, Size, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa
#         # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, Size, bool) -> Tuple[Tensor, SparseTensor]  # noqa
#         r"""
#         Args:
#             return_attention_weights (bool, optional): If set to :obj:`True`,
#                 will additionally return the tuple
#                 :obj:`(edge_index, attention_weights)`, holding the computed
#                 attention weights for each edge. (default: :obj:`None`)
#         """
#         H, C = self.heads, self.out_channels
#         edge_attr = self.bond_encoder(edge_attr)

#         x_l: OptTensor = None
#         x_r: OptTensor = None
#         if isinstance(x, Tensor):
#             assert x.dim() == 2
#             x_l = self.lin_l(x).view(-1, H, C)
#             if self.share_weights:
#                 x_r = x_l
#             else:
#                 x_r = self.lin_r(x).view(-1, H, C)
#         else:
#             x_l, x_r = x[0], x[1]
#             assert x[0].dim() == 2
#             x_l = self.lin_l(x_l).view(-1, H, C)
#             if x_r is not None:
#                 x_r = self.lin_r(x_r).view(-1, H, C)

#         assert x_l is not None
#         assert x_r is not None

#         if self.add_self_loops:
#             if isinstance(edge_index, Tensor):
#                 num_nodes = x_l.size(0)
#                 if x_r is not None:
#                     num_nodes = min(num_nodes, x_r.size(0))
#                 if size is not None:
#                     num_nodes = min(size[0], size[1])
#                 edge_index, edge_attr = remove_self_loops(
#                     edge_index, edge_attr)
#                 edge_index, edge_attr = add_self_loops(
#                     edge_index, edge_attr, fill_value=self.fill_value,
#                     num_nodes=num_nodes)
#             elif isinstance(edge_index, SparseTensor):
#                 if self.edge_dim is None:
#                     edge_index = set_diag(edge_index)
#                 else:
#                     raise NotImplementedError(
#                         "The usage of 'edge_attr' and 'add_self_loops' "
#                         "simultaneously is currently not yet supported for "
#                         "'edge_index' in a 'SparseTensor' form")

#         # propagate_type: (x: PairTensor, edge_attr: OptTensor)
#         out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr,
#                              size=size)

#         alpha = self._alpha
#         self._alpha = None

#         if self.concat:
#             out = out.view(-1, self.heads * self.out_channels)
#         else:
#             out = out.mean(dim=1)

#         if self.bias is not None:
#             out += self.bias

#         if isinstance(return_attention_weights, bool):
#             assert alpha is not None
#             if isinstance(edge_index, Tensor):
#                 return out, (edge_index, alpha)
#             elif isinstance(edge_index, SparseTensor):
#                 return out, edge_index.set_value(alpha, layout='coo')
#         else:
#             return out


#     def message(self, x_j: Tensor, x_i: Tensor, edge_attr: OptTensor,
#                 index: Tensor, ptr: OptTensor,
#                 size_i: Optional[int]) -> Tensor:
#         x = x_i + x_j

#         if edge_attr is not None:
#             if edge_attr.dim() == 1:
#                 edge_attr = edge_attr.view(-1, 1)
#             assert self.lin_edge is not None
#             edge_attr = self.lin_edge(edge_attr)
#             edge_attr = edge_attr.view(-1, self.heads, self.out_channels)
#             x += edge_attr

#         x = F.leaky_relu(x, self.negative_slope)
#         alpha = (x * self.att).sum(dim=-1)
#         alpha = softmax(alpha, index, ptr, size_i)
#         self._alpha = alpha
#         alpha = F.dropout(alpha, p=self.dropout, training=self.training)
#         return x_j * alpha.unsqueeze(-1)

#     def __repr__(self) -> str:
#         return (f'{self.__class__.__name__}({self.in_channels}, '
#                 f'{self.out_channels}, heads={self.heads})')

# ### GNN to generate node embedding
class GNN_node(torch.nn.Module):
    """
    Output:
        node representations
    """

    def __init__(self, num_layer, emb_dim, drop_ratio=0.5, JK="last", residual=False, gnn_type='gin'):
        '''
            emb_dim (int): node embedding dimensionality
            num_layer (int): number of GNN message passing layers
        '''

        super(GNN_node, self).__init__()
        self.num_layer = num_layer
        self.drop_ratio = drop_ratio
        self.JK = JK
        ### add residual connection or not
        self.residual = residual

#         if self.num_layer < 2:
#             raise ValueError("Number of GNN layers must be greater than 1.")

#         self.atom_encoder = AtomEncoder(emb_dim)

#         ###List of GNNs
#         self.convs = torch.nn.ModuleList()
#         self.batch_norms = torch.nn.ModuleList()

#         for layer in range(num_layer):
#             if gnn_type == 'gin':
#                 self.convs.append(GINConv(emb_dim))
#             elif gnn_type == 'gcn':
#                 self.convs.append(GCNConv(emb_dim))
#             else:
#                 ValueError('Undefined GNN type called {}'.format(gnn_type))

#             self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))

#     def forward(self, batched_data, perturb=None):
#         x, edge_index, edge_attr, batch = batched_data.x, batched_data.edge_index, batched_data.edge_attr, batched_data.batch

#         ### computing input node embedding
#         tmp = self.atom_encoder(x) + perturb if perturb is not None else self.atom_encoder(x)
#         h_list = [tmp]

#         for layer in range(self.num_layer):

#             h = self.convs[layer](h_list[layer], edge_index, edge_attr)
#             h = self.batch_norms[layer](h)

#             if layer == self.num_layer - 1:
#                 # remove relu for the last layer
#                 h = F.dropout(h, self.drop_ratio, training=self.training)
#             else:
#                 h = F.dropout(F.relu(h), self.drop_ratio, training=self.training)

#             if self.residual:
#                 h += h_list[layer]

#             h_list.append(h)

#         ### Different implementations of Jk-concat
#         if self.JK == "last":
#             node_representation = h_list[-1]
#         elif self.JK == "sum":
#             node_representation = 0
#             for layer in range(self.num_layer):
#                 node_representation += h_list[layer]

#         return node_representation


# ### Virtual GNN to generate node embedding
# class GNN_node_Virtualnode(torch.nn.Module):
#     """
#     Output:
#         node representations
#     """

#     def __init__(self, num_layer, emb_dim, drop_ratio=0.5, JK="last", residual=False, gnn_type='gin'):
#         '''
#             emb_dim (int): node embedding dimensionality
#         '''

#         super(GNN_node_Virtualnode, self).__init__()
#         self.num_layer = num_layer
#         self.drop_ratio = drop_ratio
#         self.JK = JK
#         ### add residual connection or not
#         self.residual = residual

#         if self.num_layer < 2:
#             raise ValueError("Number of GNN layers must be greater than 1.")

#         self.atom_encoder = AtomEncoder(emb_dim)

#         ### set the initial virtual node embedding to 0.
#         self.virtualnode_embedding = torch.nn.Embedding(1, emb_dim)
#         torch.nn.init.constant_(self.virtualnode_embedding.weight.data, 0)

#         ### List of GNNs
#         self.convs = torch.nn.ModuleList()
#         ### batch norms applied to node embeddings
#         self.batch_norms = torch.nn.ModuleList()

#         ### List of MLPs to transform virtual node at every layer
#         self.mlp_virtualnode_list = torch.nn.ModuleList()

#         for layer in range(num_layer):
#             if gnn_type == 'gin':
#                 self.convs.append(GINConv(emb_dim))
#             elif gnn_type == 'gcn':
#                 self.convs.append(GCNConv(emb_dim))
#             else:
#                 ValueError('Undefined GNN type called {}'.format(gnn_type))

#             self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))

#         for layer in range(num_layer - 1):
#             self.mlp_virtualnode_list.append(
#                 torch.nn.Sequential(torch.nn.Linear(emb_dim, 2 * emb_dim), torch.nn.BatchNorm1d(2 * emb_dim),
#                                     torch.nn.ReLU(), \
#                                     torch.nn.Linear(2 * emb_dim, emb_dim), torch.nn.BatchNorm1d(emb_dim),
#                                     torch.nn.ReLU()))

#     def forward(self, batched_data, perturb=None):

#         x, edge_index, edge_attr, batch = batched_data.x, batched_data.edge_index, batched_data.edge_attr, batched_data.batch

#         ### virtual node embeddings for graphs
#         virtualnode_embedding = self.virtualnode_embedding(
#             torch.zeros(batch[-1].item() + 1).to(edge_index.dtype).to(edge_index.device))

#         # h_list = [self.atom_encoder(x)]
#         tmp = self.atom_encoder(x) + perturb if perturb is not None else self.atom_encoder(x)
#         h_list = [tmp]

#         for layer in range(self.num_layer):
#             ### add message from virtual nodes to graph nodes
#             h_list[layer] = h_list[layer] + virtualnode_embedding[batch]

#             ### Message passing among graph nodes
#             h = self.convs[layer](h_list[layer], edge_index, edge_attr)

#             h = self.batch_norms[layer](h)
#             if layer == self.num_layer - 1:
#                 # remove relu for the last layer
#                 h = F.dropout(h, self.drop_ratio, training=self.training)
#             else:
#                 h = F.dropout(F.relu(h), self.drop_ratio, training=self.training)

#             if self.residual:
#                 h = h + h_list[layer]

#             h_list.append(h)

#             ### update the virtual nodes
#             if layer < self.num_layer - 1:
#                 ### add message from graph nodes to virtual nodes
#                 virtualnode_embedding_temp = global_add_pool(h_list[layer], batch) + virtualnode_embedding
#                 ### transform virtual nodes using MLP

#                 if self.residual:
#                     virtualnode_embedding = virtualnode_embedding + F.dropout(
#                         self.mlp_virtualnode_list[layer](virtualnode_embedding_temp), self.drop_ratio,
#                         training=self.training)
#                 else:
#                     virtualnode_embedding = F.dropout(self.mlp_virtualnode_list[layer](virtualnode_embedding_temp),
#                                                       self.drop_ratio, training=self.training)

#         ### Different implementations of Jk-concat
#         if self.JK == "last":
#             node_representation = h_list[-1]
#         elif self.JK == "sum":
#             node_representation = 0
#             for layer in range(self.num_layer):
#                 node_representation += h_list[layer]

#         return node_representation



# if __name__ == "__main__":
#     pass